{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7ff545-0e28-4916-8c14-1a40ccff2491",
   "metadata": {},
   "source": [
    "#### Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37432d4-f604-4207-ba46-c787caa66d8e",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Anomaly detection is a technique used in data mining and machine learning to identify patterns in data that do not conform to expected behavior. These patterns, or anomalies, can be indicative of errors, outliers, or significant changes in the underlying data-generating process. The purpose of anomaly detection is to detect these unusual patterns or data points, which may signify critical events, potential problems, or interesting insights.\n",
    "\n",
    "Here are a few key points about anomaly detection:\n",
    "- Identification of Outliers: Anomaly detection algorithms aim to identify outliers or data points that deviate significantly from the norm or expected behavior.\n",
    "- Problem Detection: Anomalies might signify problems such as fraud, network intrusions, equipment malfunctions, or unusual consumer behavior. Detecting these anomalies early can help mitigate risks and prevent potential damage.\n",
    "- Data Cleaning: Anomaly detection can also be used for data cleaning purposes. Identifying and removing outliers can improve the quality of the dataset for further analysis.\n",
    "- Predictive Maintenance: In industrial settings, anomaly detection can be employed for predictive maintenance. By identifying anomalies in machinery or equipment sensor data, maintenance can be scheduled proactively, reducing downtime and costs.\n",
    "- Security: In cybersecurity, anomaly detection is crucial for identifying suspicious activities or intrusions in computer networks. By analyzing network traffic, anomalies such as unusual access patterns or data transfers can be flagged for further investigation.\n",
    "- Healthcare Monitoring: Anomaly detection techniques are also used in healthcare for monitoring patient data to detect unusual medical conditions or events."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab6f7ce-e59d-4d0c-b992-cc55b3799c95",
   "metadata": {},
   "source": [
    "#### Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9bfd43-acfd-4a3e-9aaf-df7374c95f27",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Anomaly detection poses several challenges, primarily due to the diverse nature of data and the complexity of identifying unusual patterns within it. Some key challenges include:\n",
    "\n",
    "- Imbalanced Data: Anomalies are often rare events compared to normal data points, leading to imbalanced datasets. Traditional machine learning algorithms may struggle to accurately detect anomalies in such scenarios, as they are biased towards the majority class.\n",
    "- Labeling Anomalies: In many cases, anomalies may not be explicitly labeled in the dataset, making it challenging to train supervised anomaly detection models. Unsupervised or semi-supervised techniques are often used to address this challenge, but they may require substantial computational resources and expertise.\n",
    "- Data Quality Issues: Noisy data, missing values, and outliers unrelated to anomalies can obscure the true anomalies, making them difficult to detect. Preprocessing techniques such as data cleaning and normalization are essential to mitigate these issues.\n",
    "- High Dimensionality: In datasets with a high number of features or dimensions, distinguishing between normal and anomalous patterns becomes more challenging. Dimensionality reduction techniques may be applied to simplify the data while preserving relevant information.\n",
    "- Concept Drift: Anomalies may evolve over time due to changes in the underlying data-generating process, a phenomenon known as concept drift. Anomaly detection models must be adaptive to these changes to maintain their effectiveness over time.\n",
    "- Interpretability: Many anomaly detection algorithms produce black-box models that provide little insight into the reasons behind detected anomalies. Interpretable anomaly detection techniques are needed, especially in domains where understanding the cause of anomalies is crucial for decision-making.\n",
    "- Scalability: Anomaly detection algorithms should be scalable to handle large-scale datasets commonly encountered in real-world applications. Efficient algorithms and distributed computing frameworks are necessary to address scalability concerns.\n",
    "- False Positives and False Negatives: Anomaly detection algorithms must strike a balance between detecting true anomalies while minimizing false positives (normal instances incorrectly classified as anomalies) and false negatives (anomalies incorrectly classified as normal)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8e74bc-bc93-4dbd-b27a-99e1ff0db752",
   "metadata": {},
   "source": [
    "#### Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15fbd59-a4fa-447a-905e-81f19d8a6b71",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Unsupervised anomaly detection and supervised anomaly detection are two approaches used to identify anomalies in data, and they differ primarily in their reliance on labeled data during the training phase:\n",
    "\n",
    "i. Unsupervised Anomaly Detection:\n",
    "- In unsupervised anomaly detection, the algorithm works with unlabeled data, meaning it doesn't require any prior knowledge or labels indicating which instances are anomalous.\n",
    "- The algorithm identifies anomalies based solely on the inherent structure and distribution of the data. It looks for patterns that deviate significantly from the norm or expected behavior without relying on predefined labels.\n",
    "- Common unsupervised anomaly detection techniques include statistical methods (e.g., Gaussian distribution modeling, clustering), density-based approaches (e.g., DBSCAN), and proximity-based methods (e.g., k-nearest neighbors).\n",
    "\n",
    "ii. Supervised Anomaly Detection:\n",
    "- In supervised anomaly detection, the algorithm is trained on a labeled dataset containing both normal instances and anomalous instances.\n",
    "- The algorithm learns to distinguish between normal and anomalous patterns by observing the labeled examples during training. It aims to generalize from these labeled examples to identify anomalies in new, unseen data.\n",
    "- Supervised anomaly detection techniques typically involve training a classifier (e.g., decision trees, support vector machines, neural networks) on the labeled data, where anomalies are treated as the positive class and normal instances as the negative class.\n",
    "- The trained classifier is then used to predict whether new instances are normal or anomalous based on their features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e633ebe-676d-4c75-ae69-a6ce2bea116d",
   "metadata": {},
   "source": [
    "#### Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359458fc-aecd-446e-b681-b827b193d99f",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Anomaly detection algorithms can be broadly categorized into several main types, each with its own approach to identifying anomalies in data:\n",
    "\n",
    "i. Statistical Methods:\n",
    "- Statistical methods assume that normal data points follow a certain statistical distribution (e.g., Gaussian distribution), and anomalies are instances that deviate significantly from this distribution.\n",
    "- Common statistical techniques include:\n",
    "- Z-Score or Standard Score\n",
    "- Grubbs' Test\n",
    "- Dixon's Q Test\n",
    "- Generalized Extreme Studentized Deviate (GESD) Test\n",
    "- Boxplot-based methods\n",
    "\n",
    "ii. Machine Learning-Based Methods:\n",
    "- Machine learning-based methods utilize algorithms to learn patterns and relationships in the data, distinguishing between normal and anomalous instances.\n",
    "- These methods can be further divided into supervised, unsupervised, and semi-supervised approaches, as discussed earlier.\n",
    "- Supervised methods use labeled data to train classifiers to distinguish between normal and anomalous instances.\n",
    "- Unsupervised methods detect anomalies based solely on the distribution of the data without using labels.\n",
    "- Semi-supervised methods leverage a small amount of labeled data in combination with a larger set of unlabeled data.\n",
    "\n",
    "iii. Common machine learning-based techniques include:\n",
    "- Support Vector Machines (SVM)\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Decision Trees\n",
    "- Isolation Forest\n",
    "- Autoencoders (for deep learning-based anomaly detection)\n",
    "- One-class SVM\n",
    "\n",
    "iv. Density-Based Methods:\n",
    "- Density-based methods identify anomalies as instances located in low-density regions of the data space.\n",
    "- These methods typically involve estimating the density of the data distribution and flagging instances that fall below a certain threshold.\n",
    "- Common density-based techniques include:\n",
    "- DBSCAN (Density-Based Spatial Clustering of Applications with Noise)\n",
    "- LOF (Local Outlier Factor)\n",
    "- HBOS (Histogram-Based Outlier Score)\n",
    "\n",
    "v.Clustering-Based Methods:\n",
    "- lustering-based methods partition the data into groups or clusters, with anomalies being instances that do not belong to any cluster or belong to sparse clusters.\n",
    "- These methods often involve identifying clusters in the data and considering instances that are far from the cluster centroids as anomalies.\n",
    "- Common clustering-based techniques include:\n",
    "- K-Means Clustering (with outliers as instances far from cluster centroids)\n",
    "- DBSCAN (with noise points as anomalies)\n",
    "\n",
    "vi. Proximity-Based Methods:\n",
    "- Proximity-based methods identify anomalies based on the proximity or similarity of instances to their neighbors in the data space.\n",
    "- These methods flag instances that are significantly dissimilar to their nearest neighbors.\n",
    "- Common proximity-based techniques include:\n",
    "- k-Nearest Neighbors (k-NN)\n",
    "- Local Outlier Probability (LoOP)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281d60b6-1ddc-4de5-87c0-f4ddb322a7cb",
   "metadata": {},
   "source": [
    "#### Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7878cdbc-658f-40ed-ac26-dcf6ae72aa66",
   "metadata": {},
   "source": [
    "#### solve\n",
    "Distance-based anomaly detection methods rely on the assumption that anomalies are often located far from normal instances in the feature space. These methods calculate distances or similarities between data points and identify instances that are significantly distant from their neighbors or from the centroid of the data distribution. The main assumptions made by distance-based anomaly detection methods include:\n",
    "\n",
    "i. Proximity Assumption:\n",
    "- The proximity assumption suggests that normal instances are generally close to each other in the feature space, forming dense clusters or regions, while anomalies are located far from these dense regions.\n",
    "- Anomalies are assumed to be isolated points or in sparse regions of the feature space.\n",
    "\n",
    "ii. Euclidean Distance or Similarity Measure:\n",
    "- Many distance-based anomaly detection methods use the Euclidean distance metric or other similarity measures to quantify the distance or dissimilarity between data points.\n",
    "- The assumption is that anomalies have larger distances or dissimilarities compared to normal instances, making them stand out.\n",
    "\n",
    "iii. Threshold-based Detection:\n",
    "- Distance-based methods typically involve setting a threshold distance or similarity score, beyond which instances are considered anomalies.\n",
    "- The assumption is that instances exceeding this threshold are sufficiently distant from normal instances to be considered anomalies.\n",
    "\n",
    "iv.Constant Density Assumption:\n",
    "- Some distance-based methods assume a constant density of normal instances in the feature space, meaning that anomalies are identified based solely on their distance from normal instances, without considering the local density of data points.\n",
    "- However, this assumption may not hold true in all scenarios, especially in datasets with varying densities or complex structures.\n",
    "\n",
    "v. Homogeneity Assumption:\n",
    "- Distance-based methods often assume homogeneity within normal instances, meaning that normal instances share similar characteristics and are clustered tightly together.\n",
    "- Anomalies, on the other hand, are considered dissimilar to normal instances and may exhibit heterogeneous properties compared to the majority of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdc2164-8656-4006-a0fe-6a47e704cf9c",
   "metadata": {},
   "source": [
    "#### Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a19497-ba87-4536-920d-8e7bb0b72434",
   "metadata": {},
   "source": [
    "#### solve\n",
    "\n",
    "The LOF (Local Outlier Factor) algorithm computes anomaly scores by measuring the local density deviation of a data point relative to its neighbors. Here's a step-by-step explanation of how LOF computes anomaly scores:\n",
    "\n",
    "i. Neighborhood Definition:\n",
    "- For each data point p, LOF identifies its k nearest neighbors based on a distance metric(e.g., Euclidean distance).\n",
    "\n",
    "ii. Reachability Distance:\n",
    "- The reachability distance of point p with respect to its kth nearst neighbor q is defined as the maximum of the distance between p and q, and the distance between q and its kth nearest neighbor.\n",
    "- where dist(p,q) is the distance between p and q, and k-distance (q) is the distance between q and its kth nearst neighbor.\n",
    "\n",
    "iii. Local Reachability Density(LRD):\n",
    "- The local reachability density of point p is defined as inverse of the average reachability distance of p with respect to its k nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce41c36f-0a1a-4216-9950-f985b749b2d6",
   "metadata": {},
   "source": [
    "#### Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d855303e-64c8-4a75-af09-11b591305676",
   "metadata": {},
   "source": [
    "#### solve\n",
    "The Isolation Forest algorithm is a tree-based ensemble method for anomaly detection. It works by isolating anomalies in the dataset by randomly partitioning the data space into smaller subspaces. The key parameters of the Isolation Forest algorithm include:\n",
    "\n",
    "i. n_estimators:\n",
    "- This parameter determines the number of base estimators (i.e., isolation trees) to be used in the ensemble.\n",
    "- Increasing the number of estimators may improve the algorithm's performance but also increases computational complexity.\n",
    "\n",
    "ii. max_samples:\n",
    "- It specifies the maximum number of samples to be used for constructing each isolation tree.\n",
    "- Setting this parameter to a smaller value can reduce memory usage and computational time, especially for large datasets.\n",
    "\n",
    "iii. max_features:\n",
    "- This parameter controls the number of features to be considered when splitting a node in each isolation tree.\n",
    "- A smaller value for max_features can lead to more randomization and may improve the diversity of trees in the ensemble.\n",
    "\n",
    "iv. contamination:\n",
    "- This parameter specifies the expected proportion of anomalies in the dataset.\n",
    "- It is used to define the decision threshold for classifying instances as anomalies.\n",
    "- The contamination parameter is typically set based on domain knowledge or by tuning it using validation data.\n",
    "\n",
    "v. bootstrap:\n",
    "- If set to True, each isolation tree is built using a bootstrap sample of the dataset (sampling with replacement).\n",
    "- Bootstrapping introduces randomness into the training process, which can help improve the diversity of trees in the ensemble.\n",
    "\n",
    "vi. random_state:\n",
    "- This parameter controls the random seed used for generating random numbers.\n",
    "- Setting a specific random seed ensures reproducibility of results across different runs of the algorithm.\n",
    "\n",
    "vii. verbose:\n",
    "- If set to True, the algorithm prints progress messages during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b251a58d-cc98-4920-8751-d850a89d541e",
   "metadata": {},
   "source": [
    "#### Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a1e8065-dd37-4201-9361-92ab2467e3c1",
   "metadata": {},
   "source": [
    "#### solve\n",
    "To compute the anomaly score of a data point using the k-nearest neighbors (KNN) algorithm with k=10, we need to consider the relative density of the point compard to its k-nearst neighbors.\n",
    "\n",
    "In thsi case , the data point has only 2 neighbors of the same class within a radius of 0.5. since k=10, we need to consider the k nearest neighors, which are the 10 closest neighbors to the data point. However, as there are only 2 neighbors within the specified radius, we can only consider those 2 neighbors.\n",
    "\n",
    "Here's how we can compute the anomaly score.\n",
    "\n",
    "i. Compute the sixtance to the k-th nearst neighbor (10th nearest neighbor). Since there are only 2 neighbors, the distance would be the distance to the farthest neighbor anong the two.\n",
    "\n",
    "ii. The anomaly score is inversely proportional to this distance. The farther away the k-th nearst neighbor, the lower the anomaly score.\n",
    "\n",
    "Let's denote:\n",
    "- d1 as the distance to the first nearest neighbor.\n",
    "- d2 as teh distance to the second nearest neighbor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78974d35-bf92-476b-9f00-00f76c88523f",
   "metadata": {},
   "source": [
    "#### Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46ad675-3bb6-4ca1-ad73-1d3570456ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### solve\n",
    "\n",
    "In the Isolation Forest algorithm, the anomaly score for a data point is computed based on its average path length compared to the average path length of the trees in the forest. The average path length of a tree in an isolation forest is related to the depth of the tree and provides a measure of how isolated a data point is within the forest.\n",
    "\n",
    "The anomaly score for a data point is calculated using the formula:\n",
    "\n",
    "Anomaly Score = 2- average path lenght (x) / c\n",
    "\n",
    "Where:\n",
    "\n",
    "average path length\n",
    "(𝑥)\n",
    "average path length(x) is the average path length of the data point \n",
    "𝑥\n",
    "x across all trees in the forest.\n",
    "𝑐\n",
    "c is the average path length of the trees in the forest.\n",
    "Given that the dataset has 3000 data points and the isolation forest consists of 100 trees, the average path length of the trees is not directly provided. Instead, it can be estimated based on the properties of the isolation forest."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
